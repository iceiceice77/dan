---
layout: post
title: VL-BERT：PRE-TRAINING OF GENERIC VISUALLINGUISTIC REPRESENTATIONS
date: 2021-03-01
author: 观星者
tags: [sample, document]
comments: true
toc: true
pinned: true
---
# 简介

论文：https://arxiv.org/pdf/1908.08530.pdf

代码：https://github.com/jackroos/VL-BERT

本文提出了一种新型可训练通用预训练表示，用于视觉-语言任务，称为视觉-语言BERT(简称VL-BERT)。

VL-BERT采用简单且功能强大的Transformer模型作为基础，并对其进行扩展，以视觉和语言的嵌入特征作为输入。其中，作为输入的特征来自输入句子中的单词或者来自输入图像中的感兴趣区域region-of-interest (RoI)，因此模型可以适应大多数视觉-语言的后续任务。并且为了更好地实现通用表示，作者在大规模的概念标注数据集和纯文本语料库上对VL-BERT进行预训练。大量的实证分析表明，预训练的模型可以更好地对齐视觉-语言线索，有利于后续的任务，比如视觉常识推理，视觉问答和引用表达式理解。

# 模型架构 

如图，VL-BERT的模型结构其实是在BERT的基础上，在输入中嵌入一种新的视觉特征来适应视觉的相关内容。与BERT类似，模型主要由多层双向Transformer编码器组成。但与BERT只处理句子单词不同，VL-BERT把视觉元素和语言元素都作为输入，模型分别在图像的感兴趣区域(RoIs)和输入句子中的单词上定义相应特征。其实就是直接将图像、文本、segment和position embeding加和作为输入，具体如下：

## 1.Token Embedding

对于文本部分来说，正常和BERT的pretrain一样，第一位是CLS标志位，其余的都是以单词为单位。

对于视觉部分来说，只是为每个元素分配一个特殊的[IMG]标记。

## 2.Visual Feature Embedding

在结构图中可以发现，在seg A的部分（即文本部分）输入的是整张图。但是整张图片的粒度明显是大于文本token，一次性输入整张图片显然不利于图像和文本信息的交互。 所以使用了目标检测工具对图片进行分块。

论文中使用的fast rcnn提取图像的RoI（region-of-interest）。 同时为了不失掉全局信息，在[END]对应的位置又补充了整张图像。

## 3.Segment Embedding

模型定义了三种类型的片段A、B、C，将输入元素从不同的来源中进行区分。其中A和B分别表示第一个输入句子中的单词和第二个输入句子中的单词（这里我们可以像以往一样A是query，B是sku），而C表示输入图像中的RoI。

## 4.Sequence Position Embedding

同BERT的 position embedding，值得注意的是：如架构图中的 pos 7，我们认为图像中的 ROI 是没有前后顺序之分的，所以此处的 position embedding 是一致的。

# Pretrain

论文中为了更好的融合文本和视觉特征，在 mlm 时不仅对文本进行了mask操作，同时对图像进行了mask，具体如下：

## 1.Masked Language Model with visual Clues

其实这个截断与正常bert的 mask 操作一样，都是将一个文本[MASK]。如图中的例子：

原文本： kitten drink from bottle

mask后：kitten drink from [MASK]

在这里我们知道是bottle被mask掉了，如果正常的单文本角度去预测，可能猜测是bowl，可能是cup，甚至可能是toilet。

但是VL-bert不同，它是升级版的MLM，他可以借助图像的信息猜出被mask的是bottle。

## 2.Masked RoI Classification with Linguistic Clues

图像的MLM其实与语言模型差不多，如图中所示，比如mask掉的图中的cat。同理如果没有文本信息我们可以猜测是dog or baby。但是以文本信息辅助之后，我们知道是被mask掉的图为cat。

但是有不一样的地方是，在文本输入时，会同时输入整张图片，这样在mask掉cat之后，还是容易在前面整副照片中泄露出信息。所以在mask cat的时候，就同时把整副图片中的cat部分都mask掉。

（ps：这处有个疑问，为什么知道我mask的图是属于cat，已经做好了分类？）

（ps：另外是不是可以再加入特定的NSP任务（query-sku）进行pretrain？）

# fine-tune

模型通过接收<text, image>输入，通过自监督任务学习到general跨模态表示后，可以很自然的应用很多跨模态的任务中。延续原始BERT的设定，[CLS]最后输出的feature可以预测文本和图片的关系（sentence-image-relation）。

我们这边可以先以此为teacher模型，训练后表达query-title的相关性关系。最后做线上蒸馏，来压缩模型。